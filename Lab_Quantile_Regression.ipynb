{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab_lanvin_del_gallo.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "91T0I5-54xBg"
      },
      "source": [
        "# Computer Lab: Quantile Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCSewzmkciVM",
        "colab_type": "text"
      },
      "source": [
        "### Adriano Del Gallo & Alexandre Lanvin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "78gXCkYu4xBi"
      },
      "source": [
        "### 1. Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UC7J8J-84xBj",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.io import loadmat\n",
        "\n",
        "\n",
        "def load_data(filename='census_data.mat', test_size=0.33):\n",
        "    data = loadmat(filename)\n",
        "    X = data.get('A')  # Educational Attainment\n",
        "    X = np.asarray(X, dtype='float64')\n",
        "    # Variables\n",
        "#    Sex\n",
        "#    Age in 30 40\n",
        "#    Age in 40 50\n",
        "#    Age in 50 60\n",
        "#    Age in 60 70\n",
        "#    Age gte 70\n",
        "#    Non white\n",
        "#    Unmarried\n",
        "#    Education\n",
        "#    Education code squared\n",
        "    # Education\n",
        "    #00 Not in universe (Under 3 years)\n",
        "    #01 No schooling completed\n",
        "    #02 Nursery school to 4th grade\n",
        "    #03 5th grade or 6th grade\n",
        "    #04 7th grade or 8th grade\n",
        "    #05 9th grade\n",
        "    #06 10th grade\n",
        "    #07 11th grade\n",
        "    #08 12th grade, no diploma\n",
        "    #09 High school graduate\n",
        "    #10 Some college, but less than 1 year\n",
        "    #11 One or more years of college, no degree\n",
        "    #12 Associate degree\n",
        "    #13 Bachelorâ€™s degree\n",
        "    #14 Masterâ€™s degree\n",
        "    #15 Professional degree\n",
        "    #16 Doctorate degree\n",
        "    y = data.get('b')[:, 0].reshape(-1, 1)\n",
        "    del data\n",
        "    ind = np.nonzero(y == 0.)[0]  # remove samples with no income\n",
        "    X = np.delete(X, ind, axis=0)\n",
        "    y = np.delete(y, ind, axis=0)\n",
        "    X = np.delete(X, 1, axis=1)  # this variable is just ones everywhere\n",
        "\n",
        "    scaler_x = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                test_size=test_size, random_state=0)\n",
        "    X_train = scaler_x.fit_transform(X_train)\n",
        "    y_train = scaler_y.fit_transform(y_train)\n",
        "    X_test = scaler_x.transform(X_test)\n",
        "    y_test = scaler_y.transform(y_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2VWjaazw4xBq"
      },
      "source": [
        "###  2. Quantile regression with linear kernels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KJeavvJ_4xBr"
      },
      "source": [
        "#### Question 2.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3HFIzEpz4xBs"
      },
      "source": [
        "* Let us calculate $L^*_{\\tau}$, the Fenchel-Legendre conjugate of $L_{\\tau}$.  \n",
        "\n",
        "By definition: $\\forall \\phi \\in \\mathbb{R}, $ \n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{aligned}\n",
        "L^*_{\\tau}(\\phi) &= \\underset{x \\in \\mathbb{R}}{\\mathrm{sup}}(<\\phi,x> - \\,L_{\\tau}(x))\\\\\n",
        "& = \\underset{x \\in \\mathbb{R}}{\\mathrm{sup}}(\\phi x -\\mathrm{max}(-(1-\\tau)x, \\tau x)) \\\\\n",
        "& = \\underset{x \\in \\mathbb{R}}{\\mathrm{sup}} \\,\\mathrm{min} \\,((1-\\tau + \\phi)x, \\,(\\phi -\\tau) x)\n",
        "\\end{aligned}\n",
        "\\end{equation}$  \n",
        "\n",
        "If $1-\\tau + \\phi$ and $ \\phi -\\tau$ are of the same sign, then $L^* = +\\infty$. Since $\\tau$ is in $(0,1)$, the only possible case where $L^*$ is finite is when $1-\\tau + \\phi > 0$ and $ \\phi -\\tau < 0$, i.e. when $\\phi \\in [\\tau - 1,\\, \\tau]$ in which case $L^* = 0$. \n",
        "\n",
        "Finally $L^* = \\iota_{[\\tau - 1,\\, \\tau]}$ and $(\\gamma L)^* = \\iota_{[\\gamma(\\tau - 1),\\, \\gamma\\tau]}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Enfmp-L74xBt"
      },
      "source": [
        "* Let us calculate the proximal of $L^*$\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\mathrm{prox}_{\\gamma\\,L^*}(v) &=  \\underset{x \\in \\mathbb{R}}{\\mathrm{argmin}} \\, \\iota_{[\\gamma(\\tau - 1),\\, \\gamma\\tau]}(x) + \\frac{1}{2\\gamma}||v-x||^2\\\\\n",
        "& = \\underset{x \\in [\\gamma(\\tau - 1),\\, \\gamma\\tau]}{\\mathrm{argmin}} ||v-x||^2\n",
        "\\end{aligned}\n",
        "\\end{equation}$ \n",
        "\n",
        "$\\mathrm{prox}_{\\gamma L^*}(v)$ is the projection of $v$ on $d_\\tau = [\\gamma(\\tau - 1),\\, \\gamma\\tau] $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "daBVuMjG4xBt"
      },
      "source": [
        "* Finaly we can can compute $\\mathrm{prox}_{\\gamma L}$\n",
        "\n",
        "Using Moreau's identity we gat that :\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\mathrm{prox}_{\\gamma\\,L}(v) &= v  - \\mathrm{prox}_{\\gamma L^*}(v) \\\\\\\\\n",
        "prox_{\\gamma L_\\tau}(v) &= \n",
        "{\\left\\{\n",
        "    \\begin{array}{ll}\n",
        "       v-\\gamma\\tau \\,\\:\\:\\:\\: \\; \\; \\; \\; \\; \\;  \\text{if} \\;\\gamma\\tau < v   \\\\\n",
        "       0    \\;\\;\\;\\;\\; \\; \\; \\; \\; \\; \\; \\; \\;  \\; \\; \\; \\; \\text{if} \\, v\\in d_\\tau \\\\\n",
        "       v + \\gamma(\\tau-1) \\,\\: \\; \\text{if}\\; v < \\gamma (\\tau-1) \\\\\n",
        "    \\end{array}\n",
        "\\right.}\n",
        "\\end{aligned}\n",
        "\\end{equation}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_2Sfzmus4xBu"
      },
      "source": [
        "#### Question 2.2\n",
        "\n",
        "Let $(w, w_0) \\in \\mathbb{R}^2$, $\\gamma > 0$.  \n",
        "Let's calculate $prox_{\\gamma g}((w,w_0))$\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{aligned}\n",
        "prox_{\\gamma g}((w,w_0)) &= \\underset{(y,y0)\\in \\mathbb{R}^2}{\\mathrm{argmin}}\\, \\gamma g((y,y_0)) + \\frac{1}{2} ||(y,y_0) - (w,w_0)||^2 \\\\\n",
        "&= \\underset{(y,y0)\\in \\mathbb{R}^2}{\\mathrm{argmin}}\\, \\frac{\\gamma \\alpha}{2}||y||^2 + \\frac{1}{2} ||(y -w ,y_0- w_0)||^2 \\\\\n",
        "\\end{aligned}\n",
        "\\end{equation}$\n",
        "\n",
        "Let's consider the function $\\Phi_{w,w_0} : (y,y_0) \\longmapsto \\frac{\\gamma \\alpha}{2}||(y,y_0)||^2 + \\frac{1}{2} ||(y -w ,y_0- w_0)||^2$  \n",
        "$\\Phi_{w,w_0}$ is differentiable and convex on $\\mathbb{R}^2$.  \n",
        "Let $(y, y_0) \\in \\mathbb{R}^2$.\n",
        "\n",
        "$\\nabla\\Phi_{w,w_0}(y,y_0) = 0$ i.i.f. \n",
        "$\\left\\{\\begin{align}\n",
        "\\alpha \\gamma y + y-w= 0\\\\\n",
        "y_0 - w_0 =0\n",
        "\\end{align}\\right.$  \n",
        "i.e. i.i.f. $\\left\\{\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "y &= \\frac{1}{1 + \\gamma \\alpha}w\\\\\n",
        "y_0 &=w_0\n",
        "\\end{aligned}\n",
        "\\end{equation}\\right.$\n",
        "\n",
        "Therefore, $\\Phi_{w,w_0}$ admits a minimum in $\\left(\\frac{1}{1 + \\gamma \\alpha}w,w_0\\right)$, and:\n",
        "$$\\boxed{prox_{\\gamma g}((w,w_0)) = \\left(\\frac{w}{1 + \\gamma \\alpha},w_0\\right)}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uLlBQw4M4xBv"
      },
      "source": [
        "#### Question 2.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "elZL0nEH4xBx"
      },
      "source": [
        "* First equality :  \n",
        "\n",
        "$\\underset{w \\in \\mathbb{R}^d, w_0 \\in \\mathbb{R}}{\\mathrm{min}}\\frac{\\alpha}{2}||w||^2 - L_\\tau(y-xw -w_0e)$ is the primal value to the problem. Since there are feasable points, the primal value being attained is equivilant to the existence of a saddle point to the Lagrangian. \n",
        "\n",
        "Thus : \n",
        "\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\underset{w \\in \\mathbb{R}^d, w_0 \\in \\mathbb{R}}{\\mathrm{min}}\\frac{\\alpha}{2}||w||^2 - L_\\tau(y-xw -w_0e)\n",
        "&= \\underset{w \\in \\mathbb{R}^d, w_0 \\in \\mathbb{R}}{\\mathrm{min}}\\underset{z \\in \\mathbb{R}^n}{\\mathrm{max}}\\, \\mathcal{L}((w,w_0),z)\\\\\n",
        "&= \\underset{w \\in \\mathbb{R}^d, w_0 \\in \\mathbb{R}}{\\mathrm{min}}\\underset{z \\in \\mathbb{R}^n}{\\mathrm{max}} \\frac{\\alpha}{2}||w||^2 - L_\\tau^*(z) + z^T(y-xw -w_0e)\n",
        "\\end{aligned}\n",
        "\\end{equation}$ \n",
        "\n",
        "* Second equality: \n",
        "\n",
        "Since we are at a saddle point of the Lagrangian, we can permute $\\mathrm{max}$ and $\\mathrm{min}$. Minimising according to $w$ with fixed $z$ leads to the equation $z^Tx + \\alpha w^T = 0$ which solves into $w = -\\frac{x^Tz}{\\alpha}$. \n",
        "\n",
        "Minimising according to $w_0$ gives the condition $e^Tz = 0$ which we can write as $\\iota_{\\{0\\}}e^Tz$ in the Lagrangian. When injecting the value of w into the lagrangian, we get the second equality by noticing that \n",
        "$z^Txw = -\\frac{1}{\\alpha}||x^Tz||^2$\n",
        "\n",
        "\n",
        "* Last equality :\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{aligned}\n",
        "&\\underset{z \\in \\mathbb{R}^n}{\\mathrm{max}}\\,\\underset{u \\in \\mathbb{R}}{\\mathrm{min}}\\, y^Tz - \\frac{1}{2\\alpha}||x^Tz||^2 - L_\\tau^*(z) - ue^Tz\n",
        "&=\\underset{z \\in \\mathbb{R}^n}{\\mathrm{max}}\\, y^Tz - \\frac{1}{2\\alpha}||x^Tz||^2 - L_\\tau^*(z) - \\underset{u \\in \\mathbb{R}}{\\mathrm{max}}\\,ue^Tz\\\\\n",
        "&= \\underset{z \\in \\mathbb{R}^n}{\\mathrm{max}}\\, y^Tz - \\frac{1}{2\\alpha}||x^Tz||^2 - L_\\tau^*(z) - \\iota_{\\left\\{0\\right\\}}(e^Tz)\n",
        "\\end{aligned}\n",
        "\\end{equation}$  \n",
        "as when $e^Tz = 0$, then : $\\forall u \\in \\mathbb{R}, ue^Tz=0$, and else: $ue^Tz \\underset{u \\to \\mathrm{sgn}(e^Tz)\\infty}{\\longrightarrow} +\\infty$ (the limit is well defined).\n",
        "\n",
        "* Finally:\n",
        "$$\\boxed{\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\underset{w \\in \\mathbb{R}^d, w_0 \\in \\mathbb{R}}{\\mathrm{min}} \\frac{\\alpha}{2}||w||^2 + L_\\tau(y-xw -w_0e)\n",
        "&=  \\underset{w \\in \\mathbb{R}^d, w_0 \\in \\mathbb{R}}{\\mathrm{min}}\\underset{z \\in \\mathbb{R}^n}{\\mathrm{max}} \\frac{\\alpha}{2}||w||^2 - L_\\tau^*(z) + z^T(y-xw -w_0e)\\\\\n",
        "&= \\underset{z \\in \\mathbb{R}^n}{\\mathrm{max}}\\, y^Tz - \\frac{1}{2\\alpha}||x^Tz||^2 - L_\\tau^*(z) - \\iota_{\\left\\{0\\right\\}}(e^Tz)\\\\\n",
        "&= \\underset{z \\in \\mathbb{R}^n}{\\mathrm{max}}\\,\\underset{u \\in \\mathbb{R}}{\\mathrm{min}}\\, y^Tz - \\frac{1}{2\\alpha}||x^Tz||^2 - L_\\tau^*(z) - ue^Tz\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2ZWACH-E4xBy"
      },
      "source": [
        "### 3. Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aduWNiMQ8YOO"
      },
      "source": [
        "##### ADMM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CeyRsuuk4xBz"
      },
      "source": [
        "We set \n",
        "$ f(w,w_0) = \\frac{\\alpha}{2}||w||^2$\n",
        "$ g(z) = h(M(w,w_0)) =  \\sum_{i=1}^n (L_{\\tau}(y_{i} - M(w,w_0)_{i}))$\n",
        "$\\,\\text{ with } h(v) = L_\\tau(y-v) \\text{ and } M(w,w_0) = Xw + w_0e^T = (X \\; | e^T)\\begin{pmatrix} w\\\\w_0 \\end{pmatrix}$\n",
        "\n",
        "Which gives us the following iterations for the ADMM algorithm:\n",
        "\n",
        "\n",
        "$$\n",
        "\\left\\{\n",
        "    \\begin{array}{ll}\n",
        "        (w,w_0)^{k+1} \\in \\arg\\!\\min_{(w,w_0)} f((w,w_0)) + <\\lambda^k,M(w,w_0)> + \\frac{1}{2\\gamma}||M(w,w_0)-z^k||^2 \\\\\n",
        "        z^{k+1} = \\arg\\!\\min_z g(z) - <\\lambda^k,z> + \\frac{1}{2\\gamma}||Mx^{k+1}-z||^2\\\\\n",
        "        \\lambda^{k+1} = \\lambda^k + \\frac{1}{\\gamma}(Mx^{k+1} - z^{k+1}) \\\\\n",
        "    \\end{array}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "Let's compute these quantities explicitly: \n",
        "\n",
        "\n",
        "1.\n",
        "$f$ is differentiable so by the first order optimum conditions can be written\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{aligned}\n",
        "&\\alpha w + M^T (\\lambda^k -  \\frac{1}{\\gamma} z^k) + \\frac{1}{\\gamma}M^T M(w,w_0) = 0 \\\\\n",
        "\\iff& \\bigg[ \\alpha \\begin{pmatrix} I_d & 0 \\\\ 0 & 0 \\end{pmatrix} + \\frac{1}{\\gamma}M^T M \\bigg] \\,\\begin{pmatrix} w\\\\w_0 \\end{pmatrix} = M^T (\\frac{1}{\\gamma}z^k - \\lambda^k ) \\\\\n",
        "\\iff&  \\begin{pmatrix} w\\\\w_0 \\end{pmatrix} = \\bigg[ \\alpha \\begin{pmatrix} I_d & 0 \\\\ 0 & 0 \\end{pmatrix} + \\frac{1}{\\gamma}M^T M \\bigg]^{-1} M^T (\\frac{1}{\\gamma}z^k - \\lambda^k ) \\\\\n",
        "\\end{aligned}\n",
        "\\end{equation}$\n",
        "\n",
        "\n",
        "2.\n",
        "We compute $z^k$ by fermat's rule: \n",
        "\n",
        "$ \n",
        "\\begin{align}\n",
        "0 \\in \\partial \\gamma L_\\tau(y - z) - \\gamma\\lambda^k +  (z - (Mx^{k+1})) ) \n",
        "& \\iff \\gamma\\lambda^k+ (Mx^{k+1}) \\in \\partial \\gamma L_\\tau(y - z) + z \\\\\n",
        "& \\iff  z^{k+1} = \\mathrm{prox}_{\\gamma h}(\\gamma\\lambda^k + (Mx^{k+1})) \\\\\n",
        "\\end{align}\n",
        "$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_flXpv-ZoyZn"
      },
      "source": [
        "We need to compute $\\mathrm{prox}_{\\gamma h}$ \n",
        "\n",
        "For $y\\in \\mathbb{R}$,\n",
        "\\begin{align}\n",
        "\\\\prox_{\\gamma h}(v) \n",
        "& = \\arg\\min_x [\\gamma L_\\tau(y-x) +\\frac{1}{2}||x-v||^2_2]\\\\\n",
        "& = \\arg\\min_x [\\gamma \\max\\{-(1-\\tau)(y-x),\\tau(y-x)\\} +\\frac{1}{2}||x-v||^2_2]\n",
        "\\end{align}\n",
        "\n",
        "If $y<x$ :\n",
        "\n",
        "$prox_{\\gamma h}(v) = \\arg\\min_x [\\gamma\\tau(y-x)  +\\frac{1}{2}(x-v)^2]$\n",
        "\n",
        "Which gives us by the first order optimal conditions : \n",
        "\n",
        "$x= v+\\gamma\\tau$\n",
        "\n",
        "If $y>x$ : \n",
        "$ prox_{\\gamma h}(v) = \\arg\\min_x [\\gamma(\\tau-1)(y-x)  +\\frac{1}{2}(x-v)^2]$\n",
        "\n",
        "Which gives :\n",
        "\n",
        "$x= v-\\gamma (1-\\tau)$\n",
        "\n",
        "Finally,  \n",
        "$prox_{\\gamma h}(v) = \n",
        "\\left\\{\n",
        "    \\begin{array}{ll}\n",
        "       v - \\gamma(1-\\tau) \\; \\;  \\text{if} : y+\\gamma(1-\\tau)< v \\\\\n",
        "       y \\; \\; \\; \\; \\; \\; \\; \\; \\; \\;\\;\\;\\;\\;\\;\\;\\; \\text{if } v \\in [y-\\gamma \\tau, y+\\gamma(1-\\tau)]\\\\\n",
        "       v+\\gamma \\tau \\;\\;\\;\\;\\;\\;\\; \\; \\; \\;  \\text{if} : y-\\gamma \\tau>v   \n",
        "    \\end{array}\n",
        "\\right.\n",
        "$}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_WQifzCo4xB2",
        "colab": {}
      },
      "source": [
        "census_data = load_data(test_size = 0.33)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P5oS0kPP4xB6",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = census_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hWUCBK0y4xB9",
        "colab": {}
      },
      "source": [
        "def prox_h(v,y,gamma,tau):\n",
        "    if (y > v + gamma * tau):\n",
        "        return(v + gamma * tau)\n",
        "    if(y < v - gamma*(1-tau)):\n",
        "        return(v-gamma*(1-tau))\n",
        "    return y\n",
        "\n",
        "def L_tau(v, tau):\n",
        "    return max(-(1. -tau)*v, tau*v)\n",
        "\n",
        "def ADMM(X, y, n_iter=20, alpha=1, tau=0.1, gamma=1):\n",
        "    x = [[0.]]*(len(X[1])+1)\n",
        "    z = [[0.]]*(len(X))\n",
        "    Y = y\n",
        "    lambd = z\n",
        "    err_list = []\n",
        "    L_list = []\n",
        "    \n",
        "    e = [1]*len(X_train)\n",
        "    M = np.column_stack((e,X_train))\n",
        "    I = np.identity(len(M[0]))\n",
        "    I[0][0] = 0 \n",
        "    A = np.linalg.inv(alpha * I + (np.dot(M.T, M)) / gamma)\n",
        "    A = np.dot(A, M.T)\n",
        "    \n",
        "    for i in range(n_iter):\n",
        "\n",
        "        x = np.dot(A,np.subtract(z,lambd))\n",
        "        V = np.dot(M,x)+gamma*lambd\n",
        "        for j in range(len(z)):\n",
        "            z[j] = prox_h(V[j],Y[j],gamma,tau)\n",
        "        lambd +=(np.dot(M,x)-z)\n",
        "    \n",
        "        L = 0\n",
        "        for j in range (len(X)):\n",
        "            mu = 0\n",
        "            for k in range(len(X[0])):\n",
        "                mu += X[j,k]*x[k+1]\n",
        "            mu += x[0]\n",
        "            L += L_tau(Y[j][0] - mu[0], tau)\n",
        "        L_list.append(L)\n",
        "        \n",
        "    w = x[1:]\n",
        "    w0 = x[0]\n",
        "    \n",
        "    return(w, w0, L_list)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "19VxDFMm4xCA",
        "colab": {}
      },
      "source": [
        "w, w0, L_list = ADMM(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LyG-1KvqdHjs",
        "colab": {},
        "outputId": "4ce8d353-0363-4980-8c7f-529f1b8c56de"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(L_list)\n",
        "plt.title('Minimisation of the Loss')\n",
        "plt.xlabel('iteration')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2YC002RsdUaW"
      },
      "source": [
        "##### Vu-Condat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uy-8WZjpdda-"
      },
      "source": [
        "One can use $\\underset{z \\in \\mathbb{R}^n}{\\mathrm{max}}\\, y^Tz - \\frac{1}{2\\alpha}||x^Tz||^2 - L_\\tau^*(z) - \\iota_{\\left\\{0\\right\\}}(e^Tz)$ to compute our infimum (see question 2.3).  \n",
        "\n",
        "$\\underset{z \\in \\mathbb{R}^n}{\\mathrm{max}}\\, y^Tz - \\frac{1}{2\\alpha}||x^Tz||^2 - L_\\tau^*(z) - \\iota_{\\left\\{0\\right\\}}(e^Tz) = - \\,\\underset{z \\in \\mathbb{R}^n}{\\mathrm{min}}\\, -y^Tz + \\frac{1}{2\\alpha}||x^Tz||^2 + L_\\tau^*(z) + \\iota_{\\left\\{0\\right\\}}(e^Tz)$\n",
        "\n",
        "In this way, it is possible to apply the Vu-Condat algorithm for our goal with $M = e^T, f :z \\longmapsto \\frac{1}{2} \\left \\| x^T z\\right \\|^2 -y^T z, g=L^*_\\tau, h = \\iota_0$.  \n",
        "Indeed, $g,h \\in \\Gamma_0(\\mathbb{R})$ and $f$ is convex and differentiable (by theorem of operations and because the $L^2$ norm is).  \n",
        "One step from the state $k$ to the $k+1$ one may be written as follows (with notations from the course):  \n",
        "\n",
        "$$\\left\\{\n",
        "    \\begin{array}{ll}\n",
        "        z^{k+1} = \\underset{v \\in \\mathcal{Y}}{\\operatorname{argmin}} h(v) \\,- <v, \\Phi^k> + \\frac{1}{2\\gamma}\\left \\| v-Mx^k \\right \\|^2 \\\\\n",
        "       \\Phi^{k+1}=\\Phi^k+\\gamma^{-1}(Mz^k-x^{k+1}) \\\\\n",
        "       x^{k+1} = \\underset{w \\in \\mathcal{X}}{\\operatorname{argmin}} g(w)\\,+<w, \\triangledown f(x^k)> +<w, M^*(2\\Phi^{k+1}-\\Phi^k)> + \\frac{1}{2\\tau}\\left \\| w-x^k\\right \\|^2\\\\\n",
        "    \\end{array}\n",
        "\\right.\n",
        "$$  \n",
        "\n",
        "In our precise case, as $h = \\iota_0$, $h(v) \\,- <v, \\Phi^k> + \\frac{1}{2\\gamma}\\left \\| v-Mx^k \\right \\|^2 = \\left\\{\\begin{array}{ll}0\\,\\,\\mathrm{if}\\,v = 0\\\\+\\infty \\, \\mathrm{otherwise}\\end{array}\n",
        "\\right.$.  \n",
        "In this way, for all k in $\\mathbb{N}$, $\\underset{v \\in \\mathcal{Y}}{\\operatorname{argmin}} h(v) \\,- <v, \\Phi^k> + \\frac{1}{2\\gamma}\\left \\| v-Mx^k \\right \\|^2$ thus for all k in $\\mathbb{N}^*, $ $z^k = 0$.\n",
        "\n",
        "Therefore, the step may be re-written in the following way :\n",
        "\n",
        "\n",
        "$$\\left\\{\n",
        "    \\begin{array}{ll}\n",
        "       \\Phi^{k+1}=\\Phi^k-\\gamma^{-1}x^{k+1} \\\\\n",
        "       x^{k+1} = \\underset{w \\in \\mathcal{X}}{\\operatorname{argmin}} g(w)\\,+<w, \\triangledown f(x^k)> +<w, M^*(2\\Phi^{k+1}-\\Phi^k)> + \\frac{1}{2\\tau}\\left \\| w-x^k\\right \\|^2\\\\\n",
        "    \\end{array}\n",
        "\\right.\n",
        "$$  \n",
        "\n",
        "Moreover, for all k,  \n",
        "$\\begin{align}\n",
        "x^{k+1} &= \\underset{w \\in \\mathcal{X}}{\\operatorname{argmin}} g(w)\\,+<w, \\triangledown f(x^k) + M^*(2\\Phi^{k+1}-\\Phi^k)> + \\frac{1}{2\\tau}\\left \\| w-x^k\\right \\|^2 \\\\\n",
        "& = \\underset{w \\in \\mathcal{X}}{\\operatorname{argmin}} g(w)\\,+<w - x^k, \\triangledown f(x^k) + M^*(2\\Phi^{k+1}-\\Phi^k)> + \\frac{1}{2\\tau}\\left \\| w-x^k\\right \\|^2\n",
        "\\end{align}\n",
        "$  \n",
        "as $<x^k, \\triangledown f(x^k) + M^*(2\\Phi^{k+1}-\\Phi^k)>$ does not depend on $w$.  \n",
        "Thus,  \n",
        "$\\begin{align}\n",
        "x^{k+1}\n",
        "& = g(w) + \\underset{w \\in \\mathcal{X}}{\\operatorname{argmin}}\\left \\| w-\\triangledown f(x^k)+M(2\\Phi^{k+1}-\\Phi^k) \\right \\|^2 \\\\\n",
        "& = \\, prox_{\\tau L^*_\\tau}(\\triangledown f(x^k) - M(2\\Phi^{k+1}-\\Phi^k))\n",
        "\\end{align}$  \n",
        "in our precise case.\n",
        "\n",
        "Finaly, the step could be described in this way for all k:\n",
        "$$\\boxed{\\left\\{\n",
        "    \\begin{array}{ll}\n",
        "       \\Phi^{k+1}=\\Phi^k-\\gamma^{-1}x^{k+1} \\\\\n",
        "       x^{k+1} = \\, prox_{\\tau L^*_\\tau}(\\triangledown f(x^k) - M(2\\Phi^{k+1}-\\Phi^k))\\\\\n",
        "    \\end{array}\n",
        "\\right.}\n",
        "$$  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3zezSq0ciWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "census_data = load_data(test_size = 0.999)\n",
        "\n",
        "X_train, X_test, y_train, y_test = census_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BmnDNiociWf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f(z, alpha, args = (X_train, y_train)):\n",
        "    return float(np.linalg.norm(args[0].T @ z)**2/(2*alpha) - args[1].T @ z)\n",
        "\n",
        "def grad_f(z, alpha, X, y):\n",
        "    return X @ X.T @ z - y\n",
        "\n",
        "def prox_L_tau_star(z, tau):\n",
        "    return np.array([(z[i] > tau)*tau + (z[i] < tau - 1)*(tau - 1) for i in range(len(z))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uWnt7dyT4xCC",
        "colab": {}
      },
      "source": [
        "def vu_condat(X, y, f, grad_f, prox_g, M, n_iter, alpha = 1., tau = 0.7, gamma = 1.):\n",
        "    \"\"\"Vũ-Condat method, implemented in our case\n",
        "    -------------------------------------------------------\n",
        "    Inputs :\n",
        "        - X, y : dataset\n",
        "        - f: a convex differentiable function\n",
        "        - grad_f : the gradient of f (supposed to be Lipschitz)\n",
        "        - prox_g : proximity operator of g\n",
        "        - M : linear operator\n",
        "        - n_iter : number of iterations\n",
        "        - alpha, tau, gamma : constants\n",
        "    -------------------------------------------------------\n",
        "    Outputs : the infimum  \n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialization\n",
        "    phi = 1\n",
        "    x = np.zeros(y.shape)\n",
        "    evolution = []\n",
        "    \n",
        "    # Steps\n",
        "    for k in range(n_iter):\n",
        "        previous_phi = phi\n",
        "        phi += M.T @ x /gamma\n",
        "        x = prox_g(x - tau*(grad_f(x, alpha, X, y) + M @ (2*phi - previous_phi)) , tau)\n",
        "        evolution.append(f(x, alpha, args = (X, y)))\n",
        "\n",
        "    return f(x, alpha, args = (X, y)), evolution"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T3Lh7bJHdvDl",
        "colab": {}
      },
      "source": [
        "infimum, evolution = vu_condat(X_train, y_train, f= f, grad_f = grad_f, prox_g = prox_L_tau_star,M = np.ones(y_train.shape), n_iter = 15, tau = 0.7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3N9aLCFciWo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "3f67cca7-ec93-4148-f78b-00f97cf08be6"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(evolution)\n",
        "plt.title(\"Evolution for the Vu condat algorithm\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Evolution for the Vu condat algorithm')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEICAYAAABBBrPDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3de5zcdX3v8dc7u0l2E5LdQEJ2c4Eg\nRDTQNkKqWKtFEQic1mBrLbSVaBH0CKe2tUfR9hw8rSi2tV5OFQ9qSqhcRBBJbTCJiGJVkCCIbAKy\n3JrrZpOQhBBy2eRz/vh9J/wSZmY3e5vfJO/n4zGPnfn8bp+ZTOYzv+/3952vIgIzM7NKRtQ6ATMz\nKzYXCjMzq8qFwszMqnKhMDOzqlwozMysKhcKMzOryoXCkBSSTurntm+U9Phg55T2/XZJqyRtl/Sa\nITpGv5/74UDSDyS9t96PJeljkr5aZfm7Jf3nUBz7SOBCUUckPSPpxfTBWbr9yzDncMAHa0T8KCJO\nHqLD/RNwRUQcFREPDXRnQ/VBJalJ0hZJbymz7LOSbhvsY9ZCev+9tdZ5lBMRn4yI9wJImpHep421\nzutw4UJRf34vfXCWblfUOqEhdDzQ0Z8NJTUMci4VRcRO4BvAxWVyuAhYOFy5HIlcEIaeC8VhQNLo\n9I321FxsUjr7ODY9vlRSp6TNkhZJmlJhXwd8686fsku6N4V/kc5m/kjSmZJW59Z/ddrHFkkdkt6W\nW3a9pC9K+g9Jz0u6X9KJFZ7PdqAhHevJPu77WkmLJb0AvPmgfV4NvBH4lzJnYm+V9ETa7xclKbfd\nn0laKek5SUskHV/hn2Eh8AeSxuRi55L9H7sr7euAs7GU8ycq7K/0b7YyvVYrJJ020NdY0tmSHpO0\nNb0G+ed6oqTvS9okaaOkGyW1pmX/BhwH/Ht6/T5cJt8Jkr4jqTu9Xt+RNK3Cc2uQ9Jl0nKclXZE/\nC5A0Jb1PN6f37aW5bT8u6TZJX5e0DXh3in09rVJ6n25Jub4+t+0/pdyelnReLv4DSZ+Q9JO0zb9L\nOia9BtskPSBpRqV/q8NeRPhWJzfgGeCtFZYtAK7OPb4c+G66/xZgI3AaMBr4v8C9uXUDOCnd/wHw\n3tyydwP/WW7d9PhMYHW6PxLoBD4GjErHfR44OS2/HtgEvBZoBG4EbqnyfPN59WXfW4E3kH04N5XZ\n3wHPLXeM7wCtZB+E3cDctGxeOuarU75/C/ykSr6/Av409/hm4HNVXrvrgU9U2NcfAmuA3yT7MD+J\n7Ayr368xMDGt+460n78EekqvSTrG2ek9MonsAzef/zNUeP+l5ccAfwCMAcYB3wS+Xe71B94PrACm\nAROA76XXpzEtvxf4EtAEzE7/Lm9Jyz4O7AEuSP/WzSn29bR8Rn5fuffxHuBSsi8g/x1YCyiXWydw\nItCScvsV8Nb0Ot4A/GutPwNqdfMZRf35dvomWbqVvmndBFyYW++PUwzgT4AFEfHziNgFfBR4/RB8\nQzoDOAq4JiJ2R8T3yT6EL8qtc0dE/Cwiesg+xGYP4r7vjIgfR8S+yJqD+uqaiNgSEf8F3JPL6f3A\npyJiZcr3k8DsKmcVN5CanySNJys0/W12ei/wDxHxQGQ6I+JZBvYanw90RMRtEbEH+BywvrRROsay\niNgVEd3APwO/09eEI2JTRNweETsi4nng6irbvxP4fESsjojngGtKCyRNJyv4H4mInRHxMPBVDmza\n+2lEfDv9W7/YxxSfjYivRMResn+XdmBybvm/RsSTEbGV7CzwyYj4XnodvwkMyQUV9cCFov5cEBGt\nudtXUvweYIyk16UCMBu4Iy2bAjxb2kFEbCf71jl1kHObAqyKiH252LMHHWd97v4Osg+9wdr3qkPI\nNa9STscDny8VZWAz2bf7Sq/bvwFvVtas9w6yD5r+dsJPB54sEx/IazyF3GsU2Vfp/Y8lTZZ0i6Q1\nqUnn62RnIX0iaYyk/yfp2bT9vUCryvcXHZDLQfenAJtTsan0HPvzb50vijvS3fz7ryt3/8Uyj/v6\nXj3suFAcJtK3pFvJvlleBHwn9x9tLdmHHgCSxpI1E6wps6sXyJoOStoOIY21wHRJ+ffVcRWOc6j6\nsu/efgr5UH8qeRXwvoMKc3NE/KTszrNv/D8C/hR4Fy8/m9hB31/bVWTNIAcbyGu8jqwAAZD6Yqbn\nln+S7DX6tYgYT/Y8lFve2+v3IeBk4HVp+zeVDlUhl3z/RT6PtcDRksblYofyb+2fxB5kLhSHl5uA\nPyJraropF78ZeI+k2ZJGk30g3B8Rz5TZx8PA76dvhycBlxy0vAt4RYXj30/2YfhhSSMlnQn8HnBL\nP5/PYO+7Wu7lfBn4qKRTACS1SPrDXrZZCFxB1nRy40HLHgb+OHXkzqV6s85Xgb+WdLoyJ6Umr4G8\nDv8BnCLp91On8Z9zYLEaB2wHtkqaCvzPg7bv7fUbR/bNe4uko4Grqqx7K/BBSVNTh/lHSgsiYhXw\nE+BTyi49/nWy9+HXy+/qZbqBfb3kaofAhaL+lK46Kd1KzUtExP1kZwRTSFfapPj3gP8F3E72Te5E\nDuzPyPsssJvsQ2EhL/+w+ziwMDXHvDO/ICJ2k31onUfWef4l4OKIeKyfz3Ww9/154B3pqpcv9OGY\ndwCfBm5JTSmPpuNXcztwNHB3RKw7aNkH03PYQlbMv13l2N8ka+O/iawD+tvA0QN5HSJiI1kn+TVk\nTY8zgR/nVvk/ZBc8bCUrKt86aBefAv42/dv/dZlDfI6sY3kjcB/w3SrpfAVYCjwCPAQsJutY35uW\nX0TWKb2WrAn1qvQ+7lVqVroa+HHK9Yy+bGeVlXr8zcxqJl2q+uWIqHShgNWQzyjMbNhJapZ0vqTG\n1Mx1FS9dfGEF4zMKMxt2ygYm/hB4FVm/xn8AH4yIbTVNzMpyoTAzs6rc9GRmZlUddj+mNXHixJgx\nY0at0zAzqysPPvjgxoiYVG7ZYVcoZsyYwfLly2udhplZXZH0bKVlbnoyM7OqXCjMzKwqFwozM6vK\nhcLMzKpyoTAzs6pcKMzMrCoXCjMzq8qFYoj9YtUWlj+zudZpmJn1mwvFEPurWx/mf9/ZUes0zMz6\n7bAbmV0knRu282T3C7SOGVnrVMzM+s1nFENo6YpsLvctO/bw4u69vaxtZlZMLhRDaElH1/7767ft\nrGEmZmb950IxRNZv3ckvVm3ht0+aCMC6LS/WOCMzs/5xoRgiy1Kz0/zfmgHAuq0+ozCz+uRCMUSW\ndHTxioljeePM7IzCTU9mVq9cKIbA1h17uO+pTZxzShtNIxuYMGYka930ZGZ1yoViCHz/8S569gXn\nnDIZgPaWZta76cnM6pQLxRBY8mgXx44bzexprQC0tzS5j8LM6pYLxSDbuWcvP/xVN+ecMpkRIwRA\nW0sT67a66cnM6lOvhULSAkkbJD2ai31D0sPp9oykh1N8hqQXc8u+nNvmdEm/lNQp6QuSlOJHS1om\n6Yn0d0KKK63XKekRSacN/tMffD96YiMv7tnLObPa9semtDbz3I497NzjQXdmVn/6ckZxPTA3H4iI\nP4qI2RExG7gd+FZu8ZOlZRHx/lz8WuBSYGa6lfZ5JXB3RMwE7k6PAc7LrXtZ2r7wlnSsZ1xTI2e8\n4pj9sbbxTQDupzCzutRroYiIe4GyP3+azgreCdxcbR+S2oHxEXFfRARwA3BBWjwPWJjuLzwofkNk\n7gNa034Kq2fvPu5e2cVZrzqWUY0vvbTtLVmhWOvmJzOrQwPto3gj0BURT+RiJ0h6SNIPJb0xxaYC\nq3PrrE4xgMkRsS7dXw9Mzm2zqsI2B5B0maTlkpZ3d3cP4OkMzAPPPMdzO/ZwziltB8TbW5sBn1GY\nWX0aaKG4iAPPJtYBx0XEa4C/Am6SNL6vO0tnG3GoSUTEdRExJyLmTJo06VA3HzRLOtYzqnEEv/PK\nA3MoNT35yiczq0f9/plxSY3A7wOnl2IRsQvYle4/KOlJ4JXAGmBabvNpKQbQJak9ItalpqUNKb4G\nmF5hm8KJCJat6OJNMycydvSBL2vzqAZax4z0lU9mVpcGckbxVuCxiNjfpCRpkqSGdP8VZB3RT6Wm\npW2Szkj9GhcDd6bNFgHz0/35B8UvTlc/nQFszTVRFU7H2m2s2fLiy5qdSjzozszqVV8uj70Z+Clw\nsqTVki5Jiy7k5Z3YbwIeSZfL3ga8PyJKHeEfAL4KdAJPAnel+DXA2ZKeICs+16T4YuCptP5X0vaF\ntaRjPSMEZ73q2LLL21uaWLvFhcLM6k+vTU8RcVGF+LvLxG4nu1y23PrLgVPLxDcBZ5WJB3B5b/kV\nxZKO9fzmjKM55qjRZZe3tzTx8Kotw5yVmdnAeWT2IHh64wv8qms751ZodoKsUGx+YbcH3ZlZ3XGh\nGARLO7K5J86eNbniOm0tvkTWzOqTC8UgWNKxnlOmjGf60WMqrjOlxZfImll9cqEYoA3bdvLQqi1V\nm50g+2FAgPXbfImsmdUXF4oBWrayiwj2zz1RSXtqevKVT2ZWb1woBmhJRxfHHzOGkyePq7peadCd\n+yjMrN64UAzAtp17+OmTGzn3lDbSr6ZX1TbeExiZWf1xoRiAex7bwJ69wTlVrnbKa/cERmZWh1wo\nBmBpRxcTjxrNacdN6NP67a3+GQ8zqz8uFP20c89efvD4Bs6e9dKUp71pH9/EJg+6M7M640LRTz95\nciMv7N7Lub1c7ZRXukS2a5vPKsysfrhQ9NOSR7s4anQjrz/xmN5XTqakCYzcoW1m9cSFoh/27gu+\nt7KLN7/qWEY3NvR5u/2D7lwozKyOuFD0w4PPPsemF3YfUrMTeO5sM6tPLhT9sKRjPaMaXj7laW/G\njGqkpdmD7sysvrhQHKKIYOmK9bzhpGMY1zTykLfPxlK4UJhZ/XChOEQr1z3Pqs0v9vojgJW0edCd\nmdWZvkyFukDSBkmP5mIfl7RG0sPpdn5u2UcldUp6XNK5ufjcFOuUdGUufoKk+1P8G5JGpfjo9Lgz\nLZ8xWE96IJZ0rEeCs159aP0TJZ4728zqTV/OKK4H5paJfzYiZqfbYgBJs8jm0j4lbfMlSQ2SGoAv\nAucBs4CL0roAn077Ogl4DijNyX0J8FyKfzatV3NLV3Qx5/gJTBpXfsrT3rS3NLFx+2529XjQnZnV\nh14LRUTcC2zu4/7mAbdExK6IeBroBF6bbp0R8VRE7AZuAeYp+yW9twC3pe0XAhfk9rUw3b8NOEt9\n+eW9IbRq8w5WrtvW72YnyA2627prsNIyMxtSA+mjuELSI6lpqvRjR1OBVbl1VqdYpfgxwJaI6Dko\nfsC+0vKtaf2XkXSZpOWSlnd3dw/gKVW3JE15es6s/heKKS2lQXfupzCz+tDfQnEtcCIwG1gHfGbQ\nMuqHiLguIuZExJxJkw7tktVDsbSji1e1jeO4YypPedqbl2a6cz+FmdWHfhWKiOiKiL0RsQ/4ClnT\nEsAaYHpu1WkpVim+CWiV1HhQ/IB9peUtaf2a2Lh9Fw88u3lAzU6QG3Tnme7MrE70q1BIas89fDtQ\nuiJqEXBhumLpBGAm8DPgAWBmusJpFFmH96KICOAe4B1p+/nAnbl9zU/33wF8P61fE99bkU15OtBC\nMXZ0I+ObGlnvpiczqxONva0g6WbgTGCipNXAVcCZkmYDATwDvA8gIjok3QqsAHqAyyNib9rPFcAS\noAFYEBEd6RAfAW6R9AngIeBrKf414N8kdZJ1pl844Gc7AEs61jNtQjOvbq8+5WlftLc0e9CdmdWN\nXgtFRFxUJvy1MrHS+lcDV5eJLwYWl4k/xUtNV/n4TuAPe8tvOGzf1cOPOzfxrtcf36cpT3vT5tHZ\nZlZHPDK7D37w+AZ279034GankimtLhRmVj9cKPpgSUcXx4wdxenH923K0960jW9m4/Zd7O7ZNyj7\nMzMbSi4UvdjVs5d7HtvAW189mYY+Tnnam3bPdGdmdcSFohc/fXIT23f1cO6p/fttp3LaW7NC4eYn\nM6sHLhS9WNLRxdhRDfzWiRMHbZ+lMwqPzjazeuBCUcW+fcGyFV2cefKxNI3s+5SnvWlr8dzZZlY/\nXCiqeGjVc2zcvotzDnHK094cNbqRcU2N/rlxM6sLLhRVLOnoYmSDePOrjh30fbe3NLF2i5uezKz4\nXCgqiAiWdKzn9SdOZHw/pjztTVtLs38Y0MzqggtFBb/q2s6zm3Zw7iA3O5VM8ehsM6sTLhQVlKY8\nPXvW0BSKtpYmD7ozs7rgQlHB0hXrec30Vo4d1zQk+29vaSLCg+7MrPhcKMpY/dwOHl0zsClPe9Oe\nLpF1P4WZFZ0LRRlLO7oAOGdIC0VpAiNf+WRmxeZCUcbSFet55eSjOGHi2CE7RntrOqNwh7aZFZwL\nxUE2v7Cbnz098ClPe3PU6EbGjW70lU9mVnguFAf53sou9g3ClKd9kU1g5KYnMyu2XguFpAWSNkh6\nNBf7R0mPSXpE0h2SWlN8hqQXJT2cbl/ObXO6pF9K6pT0BaWp4iQdLWmZpCfS3wkprrReZzrOaYP/\n9F9uaUcXU1ubOWXK+CE/Vntrs5uezKzw+nJGcT0w96DYMuDUiPh14FfAR3PLnoyI2en2/lz8WuBS\nYGa6lfZ5JXB3RMwE7k6PAc7LrXtZ2n5I7djdw4+e6ObsWZMHZcrT3rSP96A7Myu+XgtFRNwLbD4o\ntjQietLD+4Bp1fYhqR0YHxH3RUQANwAXpMXzgIXp/sKD4jdE5j6gNe1nyPzw8W529QzelKe9aWtp\notuD7sys4Aajj+LPgLtyj0+Q9JCkH0p6Y4pNBVbn1lmdYgCTI2Jdur8emJzbZlWFbQ4g6TJJyyUt\n7+7u7vcTWbqiiwljRvKbMwZnytPeTGnNBt1teN5nFWZWXAMqFJL+BugBbkyhdcBxEfEa4K+AmyT1\nubE/nW3EoeYREddFxJyImDNp0qRD3RyAPXv3cffKLs569WQaG4anj780L4X7KcysyBr7u6GkdwO/\nC5yVPuCJiF3ArnT/QUlPAq8E1nBg89S0FAPoktQeEetS09KGFF8DTK+wzaC776lNbNvZM2zNTpAb\ndOdCYWYF1q+vzpLmAh8G3hYRO3LxSZIa0v1XkHVEP5WalrZJOiNd7XQxcGfabBEwP92ff1D84nT1\n0xnA1lwT1aBbuW4bR41u5I0zB2/K096UCsV6XyJrZgXW6xmFpJuBM4GJklYDV5Fd5TQaWJauDrov\nXeH0JuDvJO0B9gHvj4hSR/gHyK6gaibr0yj1a1wD3CrpEuBZ4J0pvhg4H+gEdgDvGcgT7c1lbzqR\nP37d8YM65WlvxjWN5CgPujOzguu1UETERWXCX6uw7u3A7RWWLQdOLRPfBJxVJh7A5b3lN5iOGt3v\nlrh+a2tpYt0WFwozKy6PzK6x9pYm1vkXZM2swFwoaqy9pcl9FGZWaC4UNdbW0syG53exZ68H3ZlZ\nMblQ1NiUltKgu121TsXMrCwXihpr8yWyZlZwLhQ1VpoSda2vfDKzgnKhqLH21tIZhQuFmRWTC0WN\njRvdyNhRDax105OZFZQLRY1Joq2lyWcUZlZYLhQFMKW12T/jYWaF5UJRAG3jPXe2mRWXC0UBtLc0\nedCdmRWWC0UBtLc2EwHdHnRnZgXkQlEApUF3bn4ysyJyoSiA9v2Fwh3aZlY8LhQF0O65s82swFwo\nCmB8UyNjRjX4ZzzMrJBcKApg/6C7be6jMLPi6VOhkLRA0gZJj+ZiR0taJumJ9HdCikvSFyR1SnpE\n0mm5bean9Z+QND8XP13SL9M2X1CaiLvSMQ5HU1o86M7MiqmvZxTXA3MPil0J3B0RM4G702OA84CZ\n6XYZcC1kH/rAVcDrgNcCV+U++K8FLs1tN7eXYxx2PHe2mRVVnwpFRNwLbD4oPA9YmO4vBC7IxW+I\nzH1Aq6R24FxgWURsjojngGXA3LRsfETcFxEB3HDQvsod47CTDbrbSY8H3ZlZwQykj2JyRKxL99cD\nk9P9qcCq3HqrU6xafHWZeLVjHEDSZZKWS1re3d3dz6dTW+0tzewL6N7uQXdmViyD0pmdzgRiMPbV\nn2NExHURMSci5kyaNGko0xgypbEUvvLJzIpmIIWiKzUbkf5uSPE1wPTcetNSrFp8Wpl4tWMcdl6a\nEtWFwsyKZSCFYhFQunJpPnBnLn5xuvrpDGBraj5aApwjaULqxD4HWJKWbZN0Rrra6eKD9lXuGIed\nKWnQnX/Gw8yKprEvK0m6GTgTmChpNdnVS9cAt0q6BHgWeGdafTFwPtAJ7ADeAxARmyX9PfBAWu/v\nIqLUQf4BsiurmoG70o0qxzjsjG9upHlkgy+RNbPC6VOhiIiLKiw6q8y6AVxeYT8LgAVl4suBU8vE\nN5U7xuFIEu2tnunOzIrHI7MLpL3FExiZWfG4UBRI23iPzjaz4nGhKJAprdlMdx50Z2ZF4kJRIG0t\nTezdF2zcvrvWqZiZ7edCUSD7B925n8LMCsSFokA8gZGZFZELRYF4SlQzKyIXigJpaR5J08gRrNvi\npiczKw4XigKRlE1gtM1nFGZWHC4UBZNNYOQzCjMrDheKgmlr8c94mFmxuFAUzJSWZrqe38XefUM6\nvYeZWZ+5UBRMadBd9/Oe6c7MisGFomBeukTW/RRmVgwuFAXjQXdmVjQuFAXz0s94uFCYWTG4UBRM\n65iRjG4cwXo3PZlZQfS7UEg6WdLDuds2SX8h6eOS1uTi5+e2+aikTkmPSzo3F5+bYp2SrszFT5B0\nf4p/Q9Ko/j/V+iCJKa2el8LMiqPfhSIiHo+I2RExGzidbH7sO9Liz5aWRcRiAEmzgAuBU4C5wJck\nNUhqAL4InAfMAi5K6wJ8Ou3rJOA54JL+5ltP2sY3uVCYWWEMVtPTWcCTEfFslXXmAbdExK6IeBro\nBF6bbp0R8VRE7AZuAeZJEvAW4La0/ULggkHKt9DaPejOzApksArFhcDNucdXSHpE0gJJE1JsKrAq\nt87qFKsUPwbYEhE9B8VfRtJlkpZLWt7d3T3wZ1Nj7a1NdG3b6UF3ZlYIAy4Uqd/gbcA3U+ha4ERg\nNrAO+MxAj9GbiLguIuZExJxJkyYN9eGGXFtLMz37go3bPejOzGpvMM4ozgN+HhFdABHRFRF7I2If\n8BWypiWANcD03HbTUqxSfBPQKqnxoPhhr32856Uws+IYjEJxEblmJ0ntuWVvBx5N9xcBF0oaLekE\nYCbwM+ABYGa6wmkUWTPWoogI4B7gHWn7+cCdg5Bv4bW3ZoXCl8iaWRE09r5KZZLGAmcD78uF/0HS\nbCCAZ0rLIqJD0q3ACqAHuDwi9qb9XAEsARqABRHRkfb1EeAWSZ8AHgK+NpB860VpdPbaLT6jMLPa\nG1ChiIgXyDqd87F3VVn/auDqMvHFwOIy8ad4qenqiDGhNOjOExiZWQF4ZHYBSaK9xWMpzKwYXCgK\nyjPdmVlRuFAUVHuLf8bDzIrBhaKg2luyQXf7POjOzGrMhaKg2luaPOjOzArBhaKg2tIlsm5+MrNa\nc6EoqJemRHWhMLPacqEoKM+dbWZF4UJRUEePHcWoxhH+uXEzqzkXioLyoDszKwoXigLLZrpz05OZ\n1ZYLRYF57mwzKwIXigJr86A7MysAF4oCa29pYs/eYOMLHnRnZrXjQlFgpXkpfOWTmdWSC0WBlcZS\neAIjM6slF4oCa2vxlKhmVnsDLhSSnpH0S0kPS1qeYkdLWibpifR3QopL0hckdUp6RNJpuf3MT+s/\nIWl+Ln562n9n2lYDzbleHDN2FKMaRrDOM92ZWQ0N1hnFmyNidkTMSY+vBO6OiJnA3ekxwHnAzHS7\nDLgWssICXAW8jmzq06tKxSWtc2luu7mDlHPhSUoTGLlQmFntDFXT0zxgYbq/ELggF78hMvcBrZLa\ngXOBZRGxOSKeA5YBc9Oy8RFxX0QEcENuX0eEtpYmd2abWU0NRqEIYKmkByVdlmKTI2Jdur8emJzu\nTwVW5bZdnWLV4qvLxA8g6TJJyyUt7+7uHujzKZQpLU2s2+Y+CjOrncZB2MdvR8QaSccCyyQ9ll8Y\nESFpSEeMRcR1wHUAc+bMOaxGp7W1NLN+6zr27QtGjDhiumfMrEAGfEYREWvS3w3AHWR9DF2p2Yj0\nd0NafQ0wPbf5tBSrFp9WJn7EKA262/TC7lqnYmZHqAEVCkljJY0r3QfOAR4FFgGlK5fmA3em+4uA\ni9PVT2cAW1MT1RLgHEkTUif2OcCStGybpDPS1U4X5/Z1RGjff4ms+ynMrDYG2vQ0GbgjXbHaCNwU\nEd+V9ABwq6RLgGeBd6b1FwPnA53ADuA9ABGxWdLfAw+k9f4uIjan+x8ArgeagbvS7YhRGp29duuL\n/Nq0lhpnY2ZHogEVioh4CviNMvFNwFll4gFcXmFfC4AFZeLLgVMHkmc9a/MZhZnVmEdmF9z+QXcu\nFGZWIy4UBTdihJjcMtoTGJlZzbhQ1IH28Z7AyMxqx4WiDrS3enS2mdWOC0UdKP2Mh2e6M7NacKGo\nA+3jm9i9dx+bd3jQnZkNPxeKOtDe6pnuzKx2XCjqwEsz3fnKJzMbfi4UdWD/oDtPYGRmNeBCUQcm\njh3NyAb5ElkzqwkXijowYoSYPL6JdW56MrMacKGoE+0tTT6jMLOacKGoE+0tze6jMLOacKGoE6Uz\niuwHeM3Mho8LRZ1oa2lid88+NnumOzMbZi4UdaI0gZH7KcxsuLlQ1InSoDsXCjMbbv0uFJKmS7pH\n0gpJHZI+mOIfl7RG0sPpdn5um49K6pT0uKRzc/G5KdYp6cpc/ARJ96f4NySN6m++9e6lubN9iayZ\nDa+BnFH0AB+KiFnAGcDlkmalZZ+NiNnpthggLbsQOAWYC3xJUoOkBuCLwHnALOCi3H4+nfZ1EvAc\ncMkA8q1rE48aTeMIsdZnFGY2zPpdKCJiXUT8PN1/HlgJTK2yyTzglojYFRFPA53Aa9OtMyKeiojd\nwC3APEkC3gLclrZfCFzQ33zrXWnQnX8Y0MyG26D0UUiaAbwGuD+FrpD0iKQFkiak2FRgVW6z1SlW\nKX4MsCUieg6KH7GyS2Td9GRmw2vAhULSUcDtwF9ExDbgWuBEYDawDvjMQI/Rhxwuk7Rc0vLu7u6h\nPlzNtLd6SlQzG34DKhSSRs+Lx3AAAAexSURBVJIViRsj4lsAEdEVEXsjYh/wFbKmJYA1wPTc5tNS\nrFJ8E9AqqfGg+MtExHURMSci5kyaNGkgT6nQPOjOzGphIFc9CfgasDIi/jkXb8+t9nbg0XR/EXCh\npNGSTgBmAj8DHgBmpiucRpF1eC+K7NPwHuAdafv5wJ39zfdw0DY+G3T33I49tU7FzI4gjb2vUtEb\ngHcBv5T0cIp9jOyqpdlAAM8A7wOIiA5JtwIryK6Yujwi9gJIugJYAjQACyKiI+3vI8Atkj4BPERW\nmI5YU1pfmsDo6LFH7JXCZjbM+l0oIuI/AZVZtLjKNlcDV5eJLy63XUQ8xUtNV0e8tpaXpkQ9dWpL\njbMxsyOFR2bXkSml0dn+FVkzG0YuFHXkmDTozhMYmdlwcqGoIw0edGdmNeBCUWc8052ZDTcXijrT\n5tHZZjbMXCjqjAfdmdlwc6GoM+0tzezq2ccWD7ozs2HiQlFnSvNSrHXzk5kNExeKOtO2fwIjd2ib\n2fBwoagzU1o9d7aZDS8Xijoz8ajRNIyQr3wys2HjQlFnGkaIyeNG+4zCzIaNC0Udam9tdh+FmQ0b\nF4o61ObR2WY2jFwo6lD7+Gx0tgfdmdlwcKGoQ+2tzezcs4+tL3rQnZkNPReKOrR/0N0WNz+Z2dBz\noahD+wfdbfMlsmY29ApfKCTNlfS4pE5JV9Y6nyKYkqZEfaJrOy/s6nFfhZkNqX7PmT0cJDUAXwTO\nBlYDD0haFBEraptZbU0aN5rRjSP41F2P8am7HqNhhBjX1Mj4ppEH/m1++ePx+x8fuM7IhsJ/ZzCz\nGil0oQBeC3RGxFMAkm4B5gFHdKFoGCFuuvQMHlu/jed39rDtxT08v7OH53fuYVv6++ymHfsfb9/V\n0+s+m0c2MK6pkaOaGmmQhuFZmNlg+/OzZvJ7vzFl0Pdb9EIxFViVe7waeN3BK0m6DLgM4Ljjjhue\nzGrs9OMncPrxE/q07t59wfadPWzbmRWU/X9f3HNAccmKTQ+Bm7LM6lFL88gh2W/RC0WfRMR1wHUA\nc+bM8afcQRpGiJYxI2kZMzRvIjM7vBW9YXoNMD33eFqKmZnZMCl6oXgAmCnpBEmjgAuBRTXOyczs\niFLopqeI6JF0BbAEaAAWRERHjdMyMzuiFLpQAETEYmBxrfMwMztSFb3pyczMasyFwszMqnKhMDOz\nqlwozMysKh1uPygnqRt4tp+bTwQ2DmI6Q62e8q2nXKG+8q2nXKG+8q2nXGFg+R4fEZPKLTjsCsVA\nSFoeEXNqnUdf1VO+9ZQr1Fe+9ZQr1Fe+9ZQrDF2+bnoyM7OqXCjMzKwqF4oDXVfrBA5RPeVbT7lC\nfeVbT7lCfeVbT7nCEOXrPgozM6vKZxRmZlaVC4WZmVXlQpFImivpcUmdkq6sdT6VSJou6R5JKyR1\nSPpgrXPqC0kNkh6S9J1a51KNpFZJt0l6TNJKSa+vdU7VSPrL9D54VNLNkppqnVOepAWSNkh6NBc7\nWtIySU+kv32bqnGIVcj1H9N74RFJd0hqrWWOJeVyzS37kKSQNHGwjudCQfYhBnwROA+YBVwkaVZt\ns6qoB/hQRMwCzgAuL3CueR8EVtY6iT74PPDdiHgV8BsUOGdJU4E/B+ZExKlkP8V/YW2zepnrgbkH\nxa4E7o6ImcDd6XERXM/Lc10GnBoRvw78CvjocCdVwfW8PFckTQfOAf5rMA/mQpF5LdAZEU9FxG7g\nFmBejXMqKyLWRcTP0/3nyT7IptY2q+okTQP+G/DVWudSjaQW4E3A1wAiYndEbKltVr1qBJolNQJj\ngLU1zucAEXEvsPmg8DxgYbq/ELhgWJOqoFyuEbE0InrSw/vIZtmsuQqvK8BngQ/D4E5870KRmQqs\nyj1eTcE/fAEkzQBeA9xf20x69TmyN+++WifSixOAbuBfUzPZVyWNrXVSlUTEGuCfyL49rgO2RsTS\n2mbVJ5MjYl26vx6YXMtkDsGfAXfVOolKJM0D1kTELwZ73y4UdUrSUcDtwF9ExLZa51OJpN8FNkTE\ng7XOpQ8agdOAayPiNcALFKdZ5GVS2/48sgI3BRgr6U9rm9Whiez6/MJfoy/pb8iafW+sdS7lSBoD\nfAz430OxfxeKzBpgeu7xtBQrJEkjyYrEjRHxrVrn04s3AG+T9AxZk95bJH29tilVtBpYHRGlM7Tb\nyApHUb0VeDoiuiNiD/At4LdqnFNfdElqB0h/N9Q4n6okvRv4XeBPorgDz04k+8Lwi/R/bRrwc0lt\ng7FzF4rMA8BMSSdIGkXWIbioxjmVJUlkbegrI+Kfa51PbyLioxExLSJmkL2u34+IQn7rjYj1wCpJ\nJ6fQWcCKGqbUm/8CzpA0Jr0vzqLAne85i4D56f584M4a5lKVpLlkzaZvi4gdtc6nkoj4ZUQcGxEz\n0v+11cBp6T09YC4UQOqsugJYQvYf7daI6KhtVhW9AXgX2Tfzh9Pt/FondRj5H8CNkh4BZgOfrHE+\nFaUzn9uAnwO/JPv/XKifnJB0M/BT4GRJqyVdAlwDnC3pCbKzomtqmWNJhVz/BRgHLEv/175c0yST\nCrkO3fGKeyZlZmZF4DMKMzOryoXCzMyqcqEwM7OqXCjMzKwqFwozM6vKhcLMzKpyoTAzs6r+P+YO\n9MMJotTpAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cIHyMFOS4xCF"
      },
      "source": [
        "#### Question 3.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8BogdJJciWr",
        "colab_type": "text"
      },
      "source": [
        "The stopping criterion we could use is to stop the algorithm when the difference between two steps in the update becomes smaller than an $\\epsilon > 0$ that we can define. We chose this because if the updates are smaller than $\\epsilon$ then we have reached a local minimum. One could also check the difference between the loss at every step. In practice however, we use a fixed number of iterations for computational reasons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "icARKBuK4xCM"
      },
      "source": [
        "#### Question 3.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS35KN2blTIb",
        "colab_type": "text"
      },
      "source": [
        "Due to memory problems and computational time we were not able to try with the full dataset. However we see on the examples that our algorithms seem to converge properly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GpDBklXciWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}